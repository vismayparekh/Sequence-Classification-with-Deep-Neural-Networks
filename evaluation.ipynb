{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import random\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Define the RNN model class (consistent with the earlier model)\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size=14, num_layers=2):  # Input size 15 and 2 layers as in the first code block\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size=64, num_layers=num_layers, dropout=0.2, batch_first=True)  # Hidden size 64, dropout 0.2\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),  # First fully connected layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),  # Second fully connected layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 5),   # Final layer with 5 output classes\n",
        "            nn.Dropout(0.1)     # Dropout for regularization\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.rnn(x)  # RNN output\n",
        "        return self.fc(x[:, -1, :])  # Fully connected layers using the last time step's output\n",
        "\n",
        "# Model parameters\n",
        "input_size = 14  # Updated to 15, to match the first block\n",
        "num_layers = 2  # Number of RNN layers\n",
        "num_classes = 5  # Number of output classes (e.g., 5 plates)\n",
        "\n",
        "# Instantiate the model (consistent with the earlier model)\n",
        "model = RNNModel(input_size=input_size, num_layers=num_layers)\n",
        "\n",
        "# Load state_dict with strict=False to avoid mismatch issues (if applicable)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('/content/rnn_trained_model (1).pth'), strict=False)\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error loading model state_dict: {e}\")\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to process input data\n",
        "def process_data(traj):\n",
        "    \"\"\"\n",
        "    Process the input trajectory data for the model.\n",
        "\n",
        "    Input:\n",
        "        traj: trajectory data containing longitude, latitude, time, and status.\n",
        "\n",
        "    Output:\n",
        "        data: processed data as a tensor suitable for model input.\n",
        "    \"\"\"\n",
        "    # Convert trajectory to DataFrame\n",
        "    df = pd.DataFrame(traj, columns=['longitude', 'latitude', 'time', 'status'])\n",
        "\n",
        "    # Convert 'time' to datetime and extract features\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "    time_features = ['month', 'day', 'dayofweek', 'hour', 'minute', 'second']\n",
        "    for feature in time_features:\n",
        "        df[feature] = getattr(df['time'].dt, feature)\n",
        "        df[f'{feature}_sin'] = np.sin(2 * np.pi * df[feature] / df[feature].max())\n",
        "        df[f'{feature}_cos'] = np.cos(2 * np.pi * df[feature] / df[feature].max())\n",
        "\n",
        "    # Drop original time column and status\n",
        "    df = df.drop(columns=['time', 'status'] + time_features)\n",
        "\n",
        "    # Normalize longitude and latitude\n",
        "    scaler = StandardScaler()\n",
        "    df[['longitude', 'latitude']] = scaler.fit_transform(df[['longitude', 'latitude']])\n",
        "\n",
        "    # Convert DataFrame to tensor\n",
        "    data = torch.FloatTensor(df.values).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    return data\n",
        "\n",
        "# Function to run the model and get predictions\n",
        "def run(data, model):\n",
        "    \"\"\"\n",
        "    Get the predicted label from the model for the processed data.\n",
        "\n",
        "    Input:\n",
        "        data: the output of the process_data function.\n",
        "        model: your trained model.\n",
        "\n",
        "    Output:\n",
        "        prediction: the predicted label (plate) of the data, an int value.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():  # Disable gradient calculations for inference\n",
        "        output = model(data)\n",
        "\n",
        "        # Get the predicted label (plate) as an int value\n",
        "        prediction = output.argmax(dim=1).item()  # Assuming classification\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Load the test data from 'test.pkl'\n",
        "with open('/content/test.pkl', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "# Evaluate model on test data\n",
        "results = []\n",
        "for trajectory in test_data:\n",
        "    processed_data = process_data(trajectory)\n",
        "    prediction = run(processed_data, model)\n",
        "    results.append(prediction)\n",
        "\n",
        "# Print results\n",
        "print(\"Predictions for test trajectories:\")\n",
        "for i, pred in enumerate(results):\n",
        "    print(f\"Trajectory {i+1}: Predicted Driver {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNC6Yge5Eu9M",
        "outputId": "e6170f12-7dff-4871-8a93-b4f3b6de7c55"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions for test trajectories:\n",
            "Trajectory 1: Predicted Driver 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-a2181916bdf8>:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/rnn_trained_model (1).pth'), strict=False)\n"
          ]
        }
      ]
    }
  ]
}